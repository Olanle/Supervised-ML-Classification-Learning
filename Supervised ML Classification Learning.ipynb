{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "_NkZeIvSyKIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code\n",
        "K-Nearest Neighbors (KNN) is a simple algorithm used for classifying data points based on their proximity to other data points. Given a new, unclassified data point, KNN identifies the 'K' closest data points (neighbors) from the training dataset. The class that appears most frequently among these 'K' neighbors is then assigned as the class of the new data point."
      ],
      "metadata": {
        "id": "2XYm36W6ynu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Counter is used to count the occurrences of elements in a list or iterable. In KNN after finding the k nearest neighbor labels Counter helps count how many times each label appears.\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "ygLO00m5yH5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#euclidean_distance is to calculate euclidean distance between points.\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((np.array(point1) - np.array(point2))**2))"
      ],
      "metadata": {
        "id": "Ivr2Y3L4zpxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "distances.append saves how far each training point is from the test point, along with its label.\n",
        "distances.sort is used to sorts the list so the nearest points come first.\n",
        "k_nearest_labels picks the labels of the k closest points.\n",
        "Uses Counter to find which label appears most among those k labels that becomes the prediction.\n",
        "\"\"\"\n",
        "\n",
        "def knn_predict(training_data, training_labels, test_point, k):\n",
        "    distances = []\n",
        "    for i in range(len(training_data)):\n",
        "        dist = euclidean_distance(test_point, training_data[i])\n",
        "        distances.append((dist, training_labels[i]))\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "    k_nearest_labels = [label for _, label in distances[:k]]\n",
        "    return Counter(k_nearest_labels).most_common(1)[0][0]"
      ],
      "metadata": {
        "id": "0k1GHpV10gAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]]\n",
        "training_labels = ['A', 'A', 'A', 'B', 'B']\n",
        "test_point = [4, 5]\n",
        "k = 3"
      ],
      "metadata": {
        "id": "hrfVFxcQ1Jzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = knn_predict(training_data, training_labels, test_point, k)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf_ldzyf1RDN",
        "outputId": "2808a9c4-7413-49fa-bed7-f38212d7e364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explaination\n",
        "\n",
        "**Training data and labels**\n",
        "\n",
        "| Point | Coordinates | Label |\n",
        "| ----- | ----------- | ----- |\n",
        "| P1    | (1, 2)      | A     |\n",
        "| P2    | (2, 3)      | A     |\n",
        "| P3    | (3, 4)      | A     |\n",
        "| P4    | (6, 7)      | B     |\n",
        "| P5    | (7, 8)      | B     |\n",
        "\n",
        "**Test point:**\n",
        "[\n",
        "T = (4, 5)\n",
        "]\n",
        "\n",
        "**k = 3**\n",
        "\n",
        "KNN will:\n",
        "\n",
        "1. Compute the distance from the test point to every training point\n",
        "2. Select the 3 closest points\n",
        "3. Assign the majority label among those neighbors\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Distance calculation (Euclidean distance)\n",
        "\n",
        "The Euclidean distance between two points ((x_1, y_1)) and ((x_2, y_2)) is:\n",
        "\n",
        "[\n",
        "d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
        "]\n",
        "\n",
        "### Distances to each training point\n",
        "\n",
        "1. **To (1, 2), label A**\n",
        "   [\n",
        "   \\sqrt{(4-1)^2 + (5-2)^2} = \\sqrt{9 + 9} = \\sqrt{18} \\approx 4.24\n",
        "   ]\n",
        "\n",
        "2. **To (2, 3), label A**\n",
        "   [\n",
        "   \\sqrt{(4-2)^2 + (5-3)^2} = \\sqrt{4 + 4} = \\sqrt{8} \\approx 2.83\n",
        "   ]\n",
        "\n",
        "3. **To (3, 4), label A**\n",
        "   [\n",
        "   \\sqrt{(4-3)^2 + (5-4)^2} = \\sqrt{1 + 1} = \\sqrt{2} \\approx 1.41\n",
        "   ]\n",
        "\n",
        "4. **To (6, 7), label B**\n",
        "   [\n",
        "   \\sqrt{(4-6)^2 + (5-7)^2} = \\sqrt{4 + 4} = \\sqrt{8} \\approx 2.83\n",
        "   ]\n",
        "\n",
        "5. **To (7, 8), label B**\n",
        "   [\n",
        "   \\sqrt{(4-7)^2 + (5-8)^2} = \\sqrt{9 + 9} = \\sqrt{18} \\approx 4.24\n",
        "   ]\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Identify the 3 nearest neighbors\n",
        "\n",
        "Sorted by distance (smallest first):\n",
        "\n",
        "| Distance | Point  | Label |\n",
        "| -------- | ------ | ----- |\n",
        "| 1.41     | (3, 4) | A     |\n",
        "| 2.83     | (2, 3) | A     |\n",
        "| 2.83     | (6, 7) | B     |\n",
        "| 4.24     | (1, 2) | A     |\n",
        "| 4.24     | (7, 8) | B     |\n",
        "\n",
        "**The 3 nearest neighbors are:**\n",
        "\n",
        "* (3, 4) ‚Üí A\n",
        "* (2, 3) ‚Üí A\n",
        "* (6, 7) ‚Üí B\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Majority vote\n",
        "\n",
        "Among the 3 neighbors:\n",
        "\n",
        "* **A:** 2 votes\n",
        "* **B:** 1 vote\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Final classification\n",
        "\n",
        "Since **A** has the majority among the nearest neighbors, the KNN classifier assigns:\n",
        "\n",
        "[\n",
        "\\boxed{\\text{Predicted label = A}}\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Intuitive interpretation\n",
        "\n",
        "The test point (4, 5) lies closer to the cluster of **A** points (which are around the line from (1,2) to (3,4)) than to the **B** cluster (which starts at (6,7)). With k = 3, the local neighborhood is dominated by class **A**, leading to that classification."
      ],
      "metadata": {
        "id": "fof6m-o86Wne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression\n",
        "\n",
        "Logistic Regression is a method used to predict the probability of a categorical outcome. Instead of predicting a continuous value, it predicts whether something belongs to a certain category (like yes/no, true/false, or 0/1). It does this by using a logistic function (also known as a sigmoid function) to squeeze the output of a linear equation between 0 and 1, representing the probability of belonging to that category. The model learns the best coefficients for the linear equation based on the training data."
      ],
      "metadata": {
        "id": "dgs3F6pX85yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Binomial Logistic regression:\n",
        "In binomial logistic regression, the target variable can only have two possible values such as \"0\" or \"1\", \"pass\" or \"fail\". The sigmoid function is used for prediction.\n",
        "\n",
        "We will be using sckit-learn library for this and shows how to use the breast cancer dataset to implement a Logistic Regression model for classification."
      ],
      "metadata": {
        "id": "ikmhodGs9L6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "\n",
        "clf = LogisticRegression(max_iter=10000, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "acc = accuracy_score(y_test, clf.predict(X_test)) * 100\n",
        "print(f\"Logistic Regression model accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "id": "eXf8a07EQh1a",
        "outputId": "9de47ea3-6bcb-4d95-b637-01d39e2e0901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy: 96.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multinomial Logistic Regression\n",
        "\n",
        "Target variable can have 3 or more possible types which are not ordered i.e types have no quantitative significance like ‚Äúdisease A‚Äù vs ‚Äúdisease B‚Äù vs ‚Äúdisease C‚Äù.\n",
        "\n",
        "In this case, the softmax function is used in place of the sigmoid function. Softmax function for K classes will be:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Here\n",
        "K represents the number of elements in the vector\n",
        "z and\n",
        "i,j iterates over all the elements in the vector."
      ],
      "metadata": {
        "id": "_wxxI63yQmg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets, linear_model, metrics\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
        "\n",
        "reg = linear_model.LogisticRegression(max_iter=10000, random_state=0)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(f\"Logistic Regression model accuracy: {metrics.accuracy_score(y_test, y_pred) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "d3XbB0l7Q5WS",
        "outputId": "7dc202e9-c40a-413b-9bb7-123bb2c4c61d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy: 96.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion can come from the module name.\n",
        "\n",
        "\"linear_model.LogisticRegression\". This can be explained as follow\n",
        "\n",
        "* linear_model is just the name of the scikit-learn module that groups models whose decision function is linear in the parameters.\n",
        "\n",
        "* LogisticRegression is a logistic regression classifier, not linear regression.\n",
        "\n",
        "Why logistic regression is in linear_model\n",
        "\n",
        "Logistic regression is considered a linear model because:\n",
        "The model computes a linear combination of the inputs:\n",
        "\n",
        "ùëß =\n",
        "ùë§\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "ùë§\n",
        "2\n",
        "ùë•\n",
        "2\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùëè\n",
        "\n",
        "That linear score is then passed through a logistic (sigmoid) function to produce probabilities.\n",
        "\n",
        "So the structure is:\n",
        "\n",
        "* Linear decision function\n",
        "\n",
        "* Logistic (nonlinear) link function\n",
        "\n",
        "Note that: This does not involve linear regression (e.g., LinearRegression) in any way."
      ],
      "metadata": {
        "id": "WXR2YMU3Q-OM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PenXG3_PRr2n"
      }
    }
  ]
}